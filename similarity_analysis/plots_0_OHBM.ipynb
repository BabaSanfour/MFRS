{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /home/hamza97/scratch/data/MFRS_data/ds000117/sub-01/ses-meg/meg/sub-01_ses-meg_task-facerecognition_run-01_meg.fif...\n",
      "    Read a total of 8 projection items:\n",
      "        mag_ssp_upright.fif : PCA-mags-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-mags-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-mags-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-mags-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-mags-v5 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-grad-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-grad-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-grad-v3 (1 x 306)  idle\n",
      "    Range : 248600 ... 788699 =    226.000 ...   716.999 secs\n",
      "Ready.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "    Read a total of 8 projection items:\n",
      "        mag_ssp_upright.fif : PCA-mags-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-mags-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-mags-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-mags-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-mags-v5 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-grad-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-grad-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-grad-v3 (1 x 306)  idle\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mne\n",
    "import pickle\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import sys\n",
    "sys.path.append(\"/../../MFRS\")\n",
    "from utils.config import similarity_folder, networks, sensors_position, mask_params, channels_grad2, channels_grad1, channels_mag\n",
    "from plot_utils import get_layers_similarity, get_bootstrap_values, extract_layers_max_sim_values\n",
    "from plot_functions import plot_MEG_topomaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"FaceNet\", \"SphereFace\", \"resnet50\", \"cornet_s\"]\n",
    "stimuli_file_names = [\"Fam\", \"Unfam\"]\n",
    "activ_types = [\"trained\", \"untrained\"]\n",
    "# Define a dictionary to map model names to indices\n",
    "model_indices = {\n",
    "    \"SphereFace\": 0,\n",
    "    \"FaceNet\": 1,\n",
    "    \"resnet50\": 2,\n",
    "    \"cornet_s\": 3\n",
    "}\n",
    "\n",
    "# Define a nested dictionary to map combinations of activ_type and stimuli_file_name to offsets\n",
    "combination_offsets = {\n",
    "    (\"trained\", \"Fam\"): 0,\n",
    "    (\"trained\", \"Unfam\"): 4,\n",
    "    (\"untrained\", \"Unfam\"): 12,\n",
    "    (\"untrained\", \"Fam\"): 8\n",
    "}\n",
    "combinations = list(itertools.product(model_names, stimuli_file_names, activ_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros( (3, 16))\n",
    "error_bars = np.zeros( (3, 16))\n",
    "\n",
    "accuracy = [87.066, 90.679, 91.856, 75.908]\n",
    "for combination in combinations:\n",
    "    model_name, stimuli_file_name, activ_type = combination\n",
    "    file = os.path.join(similarity_folder, f\"{model_name}_{stimuli_file_name}_model_sim_scores_{activ_type}_avg.pkl\")\n",
    "    with open(file, 'rb') as file_data:\n",
    "        data = pickle.load(file_data)\n",
    "    layer_similarities, extremum_values = get_layers_similarity(data, [\"model\"], correlation_measure=\"pearson\", epsilon=0.05)\n",
    "    i = model_indices.get(model_name)  \n",
    "    \n",
    "    i += combination_offsets.get((activ_type, stimuli_file_name))\n",
    "\n",
    "    results[0][i] = max(layer_similarities[\"model\"][0])\n",
    "    results[1][i] = max(layer_similarities[\"model\"][1])\n",
    "    results[2][i] = max(layer_similarities[\"model\"][2])\n",
    "\n",
    "    file = os.path.join(similarity_folder, f\"{model_name}_{stimuli_file_name}_model_bootstrap_scores_{activ_type}_avg.pkl\")\n",
    "    with open(file, 'rb') as file_data:\n",
    "        data = pickle.load(file_data)\n",
    "\n",
    "    semdata = get_bootstrap_values(data)\n",
    "    error_bars[0][i] = semdata[\"model\"][\"mag\"]\n",
    "    error_bars[1][i] = semdata[\"model\"][\"grad1\"]\n",
    "    error_bars[2][i] = semdata[\"model\"][\"grad2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original colors from the Plasma color map\n",
    "original_colors = [\n",
    "    \"#0079FF\", \"#00DFA2\", \"#F6FA70\", \"#FF0060\",\n",
    "    \"#0079FF\", \"#00DFA2\", \"#F6FA70\", \"#FF0060\",\n",
    "    \"#0079FF\", \"#00DFA2\", \"#F6FA70\", \"#FF0060\",\n",
    "    \"#0079FF\", \"#00DFA2\", \"#F6FA70\", \"#FF0060\",\n",
    "]\n",
    "\n",
    "# Define the desired lightness value (between 0 and 1)\n",
    "lightness = 0.7\n",
    "\n",
    "# Generate lighter shades of colors\n",
    "lighter_colors = []\n",
    "for color in original_colors:\n",
    "    r, g, b = colorsys.rgb_to_hls(*tuple(int(color[i:i+2], 16) / 255.0 for i in (1, 3, 5)))\n",
    "    r, g, b = colorsys.hls_to_rgb(r, lightness, b)\n",
    "    lighter_colors.append('#%02x%02x%02x' % tuple(int(c * 255) for c in (r, g, b)))\n",
    "\n",
    "# Use the lighter colors in the plot\n",
    "block_colors = lighter_colors\n",
    "\n",
    "# Define the titles for each block\n",
    "block_titles = ['Magnometers', 'Gradiometers 1', 'Gradiometers 2', 'Accuracy']\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(8, 16))\n",
    "\n",
    "# Plot each block\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(block_titles[i], fontsize=14)\n",
    "    if i != 3:\n",
    "        ax.set_ylim(0.01, 0.28)  # Adjust the y-axis limits as per data range\n",
    "\n",
    "        # Calculate total width required for each block\n",
    "        block_width = 0.8\n",
    "        num_bars = 16\n",
    "        bar_width = block_width / num_bars\n",
    "        error_values = error_bars[i] \n",
    "        for j in range(16):\n",
    "            x = j * bar_width + (0.015 * (j // 4))  # Calculate x-coordinate for each bar\n",
    "            ax.bar(x, results[i][j], color=block_colors[j], edgecolor='black', width=bar_width)\n",
    "            x = j * bar_width + (0.015 * (j // 4))  # Calculate x-coordinate for each bar\n",
    "            y = results[i][j]\n",
    "            error = error_values[j]\n",
    "            ax.errorbar(x, y, yerr=error, color='black', capsize=4, elinewidth=0.8, capthick=0.8)\n",
    "\n",
    "            # Add dotted separation line after every 4 bars\n",
    "            if (j + 1) % 4 == 0 and j != 15:\n",
    "                if j == 7:\n",
    "                    ax.axvline(x + bar_width - 0.0175, ymin=-0.3, ymax=1, color='black', linestyle=':', linewidth=1.0)\n",
    "                else:\n",
    "                    ax.axvline(x + bar_width - 0.0175, color='black', linestyle=':', linewidth=0.5)\n",
    "        \n",
    "\n",
    "        # Add agenda axis\n",
    "        for v in [0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "            ax.axhline(v, color='gray', linestyle='--', linewidth=0.4)\n",
    "        ax.axhline(y=0.2586,linewidth=2, color='#873e23')\n",
    "        ax.axhline(y=0.0176,linewidth=2, color='#e0bb41')\n",
    "\n",
    "        ax.tick_params(axis='both', which='both', length=0, labelsize=10)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_linewidth(0.5)\n",
    "        ax.spines['bottom'].set_linewidth(0.5)\n",
    "        ax.set_xticks([0.075, 0.29, 0.51, 0.73])\n",
    "        ax.set_xticklabels(['Familiar Input', 'Unfamiliar Input', 'Familiar Input', 'Unfamiliar Input'], fontsize=10)\n",
    "        ax.text(0.25, -0.2, 'Trained models', transform=ax.transAxes, ha='center', fontsize=12)\n",
    "        ax.text(0.75, -0.2, 'Untrained models', transform=ax.transAxes, ha='center', fontsize=12)\n",
    "        legend_labels = ['ShpereFace', 'FaceNet', 'ResNet', 'CORnet-S', 'Upper Noise Ceiling', 'Lower Noise Ceiling']\n",
    "        legend_colors = [    \"#0079FF\", \"#00DFA2\", \"#F6FA70\", \"#FF0060\", '#873e23', '#e0bb41']\n",
    "        legend_patches = [plt.Rectangle((0, 0), 1 if k < 4 else 0.5, 1, fc=color) for k, color in enumerate(legend_colors)]\n",
    "\n",
    "        ax.legend(legend_patches, legend_labels, loc='center right', fontsize=8)\n",
    "    else:\n",
    "        ax.set_ylim(70, 100)  # Adjust the y-axis limits as per your data range\n",
    "        # Calculate total width required for each block\n",
    "        num_bars = 4\n",
    "        bar_width = 0.8\n",
    "        \n",
    "        # Plot each bar within the block\n",
    "        for j in range(4):\n",
    "            x = j * bar_width  + (0.015 * (j // 4)) # Calculate x-coordinate for each bar\n",
    "            ax.bar(x, accuracy[j], color=block_colors[j], edgecolor='black', width=bar_width)\n",
    "\n",
    "        ax.tick_params(axis='both', which='both', length=0, labelsize=10)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_linewidth(0.5)\n",
    "        ax.spines['bottom'].set_linewidth(0.5)\n",
    "        ax.set_xticks([0, 0.8, 1.6, 2.4])\n",
    "        legend_labels = ['ShpereFace', 'FaceNet', 'ResNet', 'CORnet-S']\n",
    "\n",
    "        ax.set_xticklabels(legend_labels, fontsize=10)\n",
    "\n",
    "# Add space between subplots\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "fig.text(0.04, 0.6, 'Similarity Scores: Pearson Correlations', va='center', rotation='vertical', fontsize=12)\n",
    "fig.text(0.04, 0.2, 'Classification Accuracy', va='center', rotation='vertical', fontsize=12)\n",
    "fig.text(0.04, 0.9, 'A', va='center', fontdict=dict(fontsize=15, fontweight ='bold'))\n",
    "fig.text(0.04, 0.28, 'B', va='center', fontdict=dict(fontsize=15, fontweight ='bold'))\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FaceNet\"\n",
    "stimuli_file_names = [\"Fam\", \"Unfam\"]\n",
    "activ_types = [\"trained\", \"untrained\"]\n",
    "combinations = list(itertools.product(stimuli_file_names, activ_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_values = [[[], [], []],\n",
    "                    [[], [], []],\n",
    "                    [[], [], []],\n",
    "                    [[], [], []]]\n",
    "combination_offsets = {\n",
    "    (\"trained\", \"Fam\"): 0,\n",
    "    (\"trained\", \"Unfam\"): 1,\n",
    "    (\"untrained\", \"Unfam\"): 2,\n",
    "    (\"untrained\", \"Fam\"): 3\n",
    "}\n",
    "Names = [\"Fam trained\", \"Fam untrained\", \"Unfam trained\", \"Unfam untrained\"]\n",
    "for combination in combinations:\n",
    "    stimuli_file_name, activ_type = combination\n",
    "    file = os.path.join(similarity_folder, f\"{model_name}_{stimuli_file_name}_model_sim_scores_{activ_type}_avg.pkl\")\n",
    "    with open(file, 'rb') as file_data:\n",
    "        data = pickle.load(file_data)\n",
    "\n",
    "\n",
    "\n",
    "    layer_similarities, extremum_values = get_layers_similarity(data, [\"model\"], correlation_measure=\"pearson\", epsilon=0.05)\n",
    "    i = combination_offsets.get((activ_type, stimuli_file_name))  # Get the offset from the nested dictionary\n",
    "\n",
    "    similarity_values[i][0] = layer_similarities[\"model\"][0]\n",
    "    similarity_values[i][1] = layer_similarities[\"model\"][1]\n",
    "    similarity_values[i][2] = layer_similarities[\"model\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(16, 16))\n",
    "for i, sim_val in enumerate(similarity_values):\n",
    "    name = Names[i]\n",
    "    last = i==3\n",
    "    plot_MEG_topomaps(sim_val, (-0.13, 0.13), axes, i, name, fig, last)\n",
    "plt.tight_layout()\n",
    "fig.text(0, 1.01, 'C', va='center', fontdict=dict(fontsize=26, fontweight ='bold'))\n",
    "# Set overall plot title and labels\n",
    "plt.suptitle('FaceNet plots', fontsize=16, y=1.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"FaceNet\", \"SphereFace\", \"resnet50\", \"cornet_s\"]\n",
    "stimuli_file_names = [\"Fam\", \"Unfam\"]\n",
    "activ_types = [\"trained\", \"untrained\"]\n",
    "# Define a dictionary to map model names to indices\n",
    "model_indices = {\n",
    "    \"SphereFace\": 0,\n",
    "    \"FaceNet\": 1,\n",
    "    \"resnet50\": 2,\n",
    "    \"cornet_s\": 3\n",
    "}\n",
    "\n",
    "# Define a nested dictionary to map combinations of activ_type and stimuli_file_name to offsets\n",
    "combination_offsets = {\n",
    "    (\"trained\", \"Fam\"): 0,\n",
    "    (\"trained\", \"Unfam\"): 1,\n",
    "    (\"untrained\", \"Unfam\"): 2,\n",
    "    (\"untrained\", \"Fam\"): 3\n",
    "}\n",
    "combinations = list(itertools.product(model_names, stimuli_file_names, activ_types))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [np.zeros( (4, len(networks[\"SphereFace\"]))),\n",
    "            np.zeros( (4, len(networks[\"FaceNet\"]))),\n",
    "            np.zeros( (4, len(networks[\"resnet50\"]))),\n",
    "            np.zeros( (4, len(networks[\"cornet_s\"])))]\n",
    "error_bars = [np.zeros( (4, len(networks[\"SphereFace\"]))),\n",
    "            np.zeros( (4, len(networks[\"FaceNet\"]))),\n",
    "            np.zeros( (4, len(networks[\"resnet50\"]))),\n",
    "            np.zeros( (4, len(networks[\"cornet_s\"])))]\n",
    "max_idx = np.zeros((4, 4))\n",
    "needed_layers = {}\n",
    "masks = [[[], [], [], [],], [[], [], [], [],], [[], [], [], [],], [[], [], [], [],],]\n",
    "highest_layers_sims = np.zeros( (4, 4, 102))\n",
    "accuracy = [87.066, 90.679, 91.856, 75.908]\n",
    "for combination in combinations:\n",
    "    model_name, stimuli_file_name, activ_type = combination\n",
    "    model_error_bars = np.zeros( (4, len(networks[model_name])))\n",
    "    file = os.path.join(similarity_folder, f\"{model_name}_{stimuli_file_name}_main_sim_scores_{activ_type}_avg.pkl\")\n",
    "    with open(file, 'rb') as file_data:\n",
    "        data = pickle.load(file_data)\n",
    "    layer_similarities, extremum_values = get_layers_similarity(data, networks[model_name])\n",
    "    i = model_indices.get(model_name)  # Get the index from the dictionary, \n",
    "    values_list, max_ind, max_layer_name, mask = extract_layers_max_sim_values(layer_similarities, \"grad2\", channels_grad2)\n",
    "    j = combination_offsets.get((activ_type, stimuli_file_name)) \n",
    "    if stimuli_file_name == \"Fam\" and activ_type == \"trained\":\n",
    "        needed_layers[model_name] = max_layer_name\n",
    "    results[i][j] = values_list\n",
    "    masks[i][j] = mask\n",
    "    max_idx[i][j] = int(max_ind)\n",
    "    highest_layers_sims[j][i] = layer_similarities[networks[model_name][max_ind]][2]\n",
    "    file = os.path.join(similarity_folder, f\"{model_name}_{stimuli_file_name}_main_bootstrap_scores_{activ_type}_avg.pkl\")\n",
    "    with open(file, 'rb') as file_data:\n",
    "        data = pickle.load(file_data)\n",
    "    semdata = get_bootstrap_values(data)\n",
    "    for k, (layer, boot_values) in enumerate(semdata.items()):\n",
    "        error_bars[i][j][k] = boot_values[\"grad2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Familiar Input / Trained model\", \"Unfamiliar Input / Trained model\", \"Familiar Input / Untrained model\", \"Unfamiliar Input / Untrained model\"]  # Add labels for the horizontal lines\n",
    "subplot_titles = [\"SphereFace\", \"FaceNet\", \"ResNet50\", \"CORnet-S\"]\n",
    "\n",
    "\n",
    "# Create the figure and subplots (5 rows, 4 columns)\n",
    "fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(30, 25))\n",
    "\n",
    "# Plot existing data in the first row\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    ax.set_title(subplot_titles[i], fontsize=14)\n",
    "    \n",
    "    # Get the number of layers for the current array\n",
    "    n_layers = results[i].shape[1]\n",
    "\n",
    "    # Define the x-axis values (layer numbers)\n",
    "    x = np.arange(1, n_layers + 1)\n",
    "\n",
    "    # Get the index of the maximum value for each layer (column) in the current array\n",
    "    max_ind = np.argmax(results[i], axis=1)[0]\n",
    "    colors = []\n",
    "    # Plot four lines for the four rows in the current array\n",
    "    for j in range(4):\n",
    "        line, = ax.plot(x, results[i][j], label=labels[j])\n",
    "        \n",
    "        # Calculate the upper and lower bounds for the error region\n",
    "        upper_bound = results[i][j] + error_bars[i][j]\n",
    "        lower_bound = results[i][j] - error_bars[i][j]\n",
    "\n",
    "        # Fill the area between the upper and lower bounds to create the shaded error region\n",
    "        ax.fill_between(x, lower_bound, upper_bound, color=line.get_color(), alpha=0.3)\n",
    "\n",
    "        # Mark the maximum value with a star marker\n",
    "        ax.plot(x[int(max_idx[i][j])], results[i][j][int(max_idx[i][j])], marker='*', markersize=16, color=line.get_color(), markeredgecolor='k')\n",
    "        colors.append(line.get_color())\n",
    "    # Add horizontal lines to the plot\n",
    "    ax.axhline(y=0.2586, linewidth=1, color='#873e23')  # Change linewidth to 1 for thin lines\n",
    "    ax.axhline(y=0.0176, linewidth=1, color='#e0bb41')  # Change linewidth to 1 for thin lines\n",
    "\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel('Layer Number', fontsize=12)\n",
    "\n",
    "    # Set y-axis label\n",
    "    ax.set_ylabel('Similarity Scores: Pearson Correlations', fontsize=12)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    # Set grid\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5)\n",
    "    ax.set_ylim(0.01, 0.26)  # Adjust the y-axis limits as per your data range\n",
    "\n",
    "    # Add legend to each subplot\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "# Remove the rest of the empty subplots\n",
    "for row in range(1, 5):\n",
    "    for col in range(4):\n",
    "        # fig.delaxes(axes[row, col])\n",
    "        mask_params[\"markerfacecolor\"] = colors[row-1]\n",
    "        im, _ = mne.viz.plot_topomap(highest_layers_sims[row-1, col], sensors_position, show=False, vlim=(-0.17,0.17),\n",
    "                                 sphere=0.18, mask=np.array(masks[row-1][col]), mask_params=mask_params, axes=axes[row, col], extrapolate='head')\n",
    "        axes[4][col].set_xlabel(subplot_titles[col], fontsize=16)\n",
    "    axes[row][0].set_ylabel(labels[row-1], fontsize=16)\n",
    "    fig.colorbar(im, ax=axes[row][3], orientation='vertical')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.text(0, 1.01, 'A', va='center', fontdict=dict(fontsize=26, fontweight ='bold'))\n",
    "fig.text(0., 0.78, 'B', va='center', fontdict=dict(fontsize=26, fontweight ='bold'))\n",
    "# Set overall plot title and labels\n",
    "plt.suptitle('Grad2 plots', fontsize=16, y=1.02)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"FaceNet\", \"SphereFace\", \"resnet50\", \"cornet_s\"]\n",
    "frequency_names = [ \"alpha\", \"beta\", \"theta\", \"delta\", \"Gamma\", ]\n",
    "# Define a dictionary to map model names to indices\n",
    "model_indices = {\n",
    "    \"SphereFace\": 0,\n",
    "    \"FaceNet\": 1,\n",
    "    \"resnet50\": 2,\n",
    "    \"cornet_s\": 3\n",
    "}\n",
    "\n",
    "# Define a nested dictionary to map combinations of activ_type and stimuli_file_name to offsets\n",
    "frequency_indices = {\n",
    "    \"alpha\": 0,\n",
    "    \"beta\": 1,\n",
    "    \"theta\": 2,\n",
    "    \"delta\": 3,\n",
    "    \"Gamma\": 4,\n",
    "\n",
    "}\n",
    "combinations = list(itertools.product(model_names, frequency_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_frequency_sims = np.zeros( (5, 4, 102))\n",
    "for combination in combinations:\n",
    "    model_name, frequency_name = combination\n",
    "    file = os.path.join(similarity_folder, f\"{model_name}_Fam_{frequency_name}_main_sim_scores_trained_avg.pkl\")\n",
    "    with open(file, 'rb') as file_data:\n",
    "        data = pickle.load(file_data)\n",
    "    layer_similarities, extremum_values = get_layers_similarity(data, networks[model_name])\n",
    "    i = model_indices.get(model_name)  \n",
    "    j = frequency_indices.get(frequency_name)  \n",
    "    layers_frequency_sims[j][i] = np.array(layer_similarities[needed_layers[model_name]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_titles = [\"SphereFace\", \"FaceNet\", \"ResNet50\", \"CORnet-S\"]\n",
    "\n",
    "\n",
    "# Create the figure and subplots (5 rows, 4 columns)\n",
    "fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(30, 25))\n",
    "\n",
    "for row in range(5):\n",
    "    for col in range(4):\n",
    "        im, _ = mne.viz.plot_topomap(layers_frequency_sims[row, col], sensors_position, show=False, vlim=(-0.046,0.046),\n",
    "                                 sphere=0.18, axes=axes[row, col], extrapolate='head')\n",
    "        axes[4][col].set_xlabel(subplot_titles[col], fontsize=16)\n",
    "    axes[row][0].set_ylabel(frequency_names[row], fontsize=16)\n",
    "    fig.colorbar(im, ax=axes[row][3], orientation='vertical')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "# Set overall plot title and labels\n",
    "plt.suptitle('Familiar Input, Trained model, Grad2 plots', fontsize=16, y=1.02)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FaceNet\"\n",
    "stimuli_file = \"Fam\"\n",
    "activ_type = \"trained\"\n",
    "across_time_folder = os.path.join(similarity_folder, f\"across_time/Fam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((102, 176))\n",
    "for i in range(176):\n",
    "    file = os.path.join(across_time_folder, f\"{model_name}_{i:03d}_{stimuli_file}_model_sim_scores_{activ_type}_avg.pkl\")\n",
    "    with open(file, 'rb') as file_data:\n",
    "        data = pickle.load(file_data)\n",
    "    for j, sensor in enumerate(channels_grad1):\n",
    "        results[j][i]=data[f\"model {sensor}\"][\"pearson\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "# Threshold value for filtering rows\n",
    "threshold = 0.04\n",
    "\n",
    "# Find rows with values above the threshold\n",
    "rows_above_threshold = np.where(np.any(results > threshold, axis=1))[0]\n",
    "\n",
    "# Find the index of the row that contains the maximum value among the selected rows\n",
    "max_row_index = rows_above_threshold[np.argmax(np.max(results[rows_above_threshold], axis=1))]\n",
    "\n",
    "# Calculate the average and standard deviation of the rows above the threshold\n",
    "average_row = np.mean(results[rows_above_threshold], axis=0)\n",
    "std_row = np.std(results[rows_above_threshold], axis=0)\n",
    "\n",
    "# Create a time array for the x-axis (176 * 5 data points, 5 seconds interval)\n",
    "time_array = np.arange(0, 176 * 5, 5)\n",
    "\n",
    "# Get the index corresponding to 800 seconds (time index 160)\n",
    "time_index_800_seconds = 160\n",
    "\n",
    "# Slice the time array to stop at 800 seconds\n",
    "time_array_800_seconds = time_array[:time_index_800_seconds + 1]\n",
    "# Create a new time array with more data points for smoother lines\n",
    "new_time_array = np.linspace(0, time_index_800_seconds * 5, 1000)\n",
    "\n",
    "# Interpolate the average and standard deviation data for the new time array\n",
    "f_average = interp1d(time_array, average_row, kind='cubic')\n",
    "f_std = interp1d(time_array, std_row, kind='cubic')\n",
    "\n",
    "# Calculate the interpolated average and standard deviation values\n",
    "smoothed_average_row = f_average(new_time_array)\n",
    "smoothed_std_row = f_std(new_time_array)\n",
    "\n",
    "# Create a bigger figure\n",
    "plt.figure(figsize=(24, 16))\n",
    "\n",
    "# Plot the interpolated average of the rows above the threshold\n",
    "plt.plot(new_time_array, smoothed_average_row, label='Average of Sensors', color='b')\n",
    "\n",
    "\n",
    "# Interpolate the row with the maximum value for the new time array\n",
    "f_max_row = interp1d(time_array, results[max_row_index], kind='cubic')\n",
    "smoothed_max_row = f_max_row(new_time_array)\n",
    "\n",
    "# Fill the area between one standard deviation below and above the interpolated average line\n",
    "plt.fill_between(new_time_array, smoothed_average_row - smoothed_std_row, smoothed_average_row + smoothed_std_row, alpha=0.2, color='b')\n",
    "plt.plot(new_time_array, smoothed_max_row, label=f'MEG2543 sensor (Maximum Sensor Similarity Score)', color='#65aeff')\n",
    "\n",
    "# Add a vertical line at the position of the maximum value\n",
    "max_value_time_index = np.argmax(results[rows_above_threshold].max(axis=0))\n",
    "\n",
    "# Add a vertical line at the position of the maximum value\n",
    "max_value_time_index = np.argmax(results[max_row_index])\n",
    "plt.axvline(x=170, color='r', linestyle='--', label=f'Face Recognition F170 Component')\n",
    "\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Similarity Scores: Pearson Correlations')\n",
    "plt.title('Familiar Input, trained FaceNet, Grad2, across time analysis')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

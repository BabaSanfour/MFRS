{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43604d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../MFRS')\n",
    "\n",
    "from utils.config import proj_path, data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f07d8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "face_encoder = dlib.face_recognition_model_v1(\"dlib_face_recognition_resnet_model_v1.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162427fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def face_distance(face_encodings, face_to_compare):\n",
    "    return np.linalg.norm(face_encodings - face_to_compare)\n",
    "\n",
    "\n",
    "def compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\n",
    "    compare = []\n",
    "    torf = (face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)\n",
    "    dis = np.round(face_distance(known_face_encodings, face_encoding_to_check), 2)\n",
    "    return [torf, dis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ca4f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitmuli_colors = os.path.join(data_path, 'STIMULI_colors/')\n",
    "pictures_pathes = sorted(\n",
    "    [\n",
    "        os.path.join(sitmuli_colors, sname)\n",
    "        for sname in os.listdir(sitmuli_colors)\n",
    "    ]\n",
    ")\n",
    "all_data = os.path.join(data_path, 'all/')\n",
    "all_data = sorted(\n",
    "    [\n",
    "        os.path.join(all_data, sname)\n",
    "        for sname in os.listdir(all_data)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0d6f4043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49026/49026 [1:36:21<00:00,  8.48it/s]   \n"
     ]
    }
   ],
   "source": [
    "pictures_loop_generator = tqdm(all_data)\n",
    "final_list = []\n",
    "for picture in pictures_loop_generator:\n",
    "    img = cv2.imread(picture)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_face = detector(img_gray)[0]\n",
    "\n",
    "    landmarks = predictor(img_gray, img_face)\n",
    "    face_embedding = np.array(face_encoder.compute_face_descriptor(img_face, landmarks, num_jitters=1))\n",
    "    final_list.append(face_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "16dcd70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-128-e509dbf68953>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(\"celebA_embeddings.npy\", np.array(final_list))\n"
     ]
    }
   ],
   "source": [
    "np.save(\"celebA_embeddings.npy\", np.array(final_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d8cb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "id=0 # iterate manually from 0 to 149\n",
    "img = cv2.imread(pictures_pathes[0])\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "face = detector(gray)[0]\n",
    "landmarks = predictor(gray, face)\n",
    "main_face_embedding = np.array(face_encoder.compute_face_descriptor(im, landmarks, num_jitters=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "670429b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comp = []\n",
    "for i, list_of_face_embedding in enumerate(final_list):\n",
    "    if len(list_of_face_embedding)==0:\n",
    "        list_comp.append([i, -1])\n",
    "        continue\n",
    "    listof = compare_faces(list_of_face_embedding[0], main_face_embedding, tolerance=0.6)\n",
    "    list_comp.append(listof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a4172fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score, max_score = 0.0, 0.5\n",
    "selected=[]\n",
    "for i, item in enumerate(list_comp):\n",
    "    if type(item[0])==int:\n",
    "        continue\n",
    "    if  item[1]>=min_score and item[1]<max_score:\n",
    "        selected.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc80668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pictures and verify \n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(len(selected),1, figsize=(15, len(selected)*5))\n",
    "for i in range(len(selected)):\n",
    "    im=plt.imread(all_data[selected[i]])\n",
    "    axes[i].imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f129ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = {4:[12951, 33166, 15094],\n",
    "          5: [9048],\n",
    "          8: [5364, 3574, 15500],\n",
    "          11:[16119],\n",
    "          16: [6203],\n",
    "          19: [2910, 15246, 31560 ],\n",
    "          20: [7522],\n",
    "          29: [9819, 25086],\n",
    "           36: [46476],\n",
    "           101: [34937],\n",
    "           102: [25683],\n",
    "           105:[20983],\n",
    "           107:[31166],\n",
    "           109: [25198, 10701],\n",
    "           114: [19497],\n",
    "           123: [33688], \n",
    "           125: [6155],\n",
    "           130: [45260, 48905],\n",
    "           136: [39948],\n",
    "           138: [41123],\n",
    "           140: [28371],\n",
    "           143: [11257],\n",
    "           52: [1007],\n",
    "           53: [44424],\n",
    "           57: [2926],\n",
    "           62: [1105],\n",
    "           65: [6577],\n",
    "           68: [5539],\n",
    "           69: [23033],\n",
    "           80: [27486],\n",
    "           81: [17062],\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bbc291b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../../files/identity_CelebA.txt', sep=\" \", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dfe24de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_ids = {}\n",
    "for key, value in matched.items():\n",
    "    name = os.path.basename(all_data[value[0]])\n",
    "    id = list(data[data[0]==name][1])[0]\n",
    "    occ=data[data[1]==id].count()[0]\n",
    "    name = f'f{key:03d}'\n",
    "    matched_ids[name] = [id, occ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f01165d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_name = os.path.join(data_path, 'web_scrapping_stimuli/')\n",
    "stimuli_name = sorted(\n",
    "    [\n",
    "        sname for sname in os.listdir(stimuli_name)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b9c965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c596b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_images = len(data)\n",
    "for stimuli in matched_ids.keys():\n",
    "    values = matched_ids[stimuli]\n",
    "    if values[1]==30:\n",
    "        continue\n",
    "    scrapped_data_folder = os.path.join(data_path, 'web_scrapping_stimuli/', stimuli)\n",
    "    scrapped_data_pictures = sorted(\n",
    "    [\n",
    "        os.path.join(scrapped_data_folder, sname)\n",
    "        for sname in os.listdir(scrapped_data_folder)\n",
    "    ]\n",
    "    )\n",
    "    for image in scrapped_data_pictures:\n",
    "        if os.path.basename(image) == \".DS_Store\":\n",
    "            continue\n",
    "        im = Image.open(image)\n",
    "        im = im.convert(\"RGB\")\n",
    "        im.save(os.path.join(data_path, f'new_celebA/{counter_images}.jpg'))\n",
    "        list_row = [ f'{counter_images}.jpg', values[0]]\n",
    "        data.loc[len(data)] = list_row\n",
    "        counter_images+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82ea9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_ids = [values[0] for values in matched_ids.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "20576af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_images = len(data)\n",
    "new_id = max(list(data[1]))+1\n",
    "for stimuli in stimuli_name:\n",
    "    if stimuli in matched_ids.keys():\n",
    "        continue\n",
    "    scrapped_data_folder = os.path.join(data_path, 'web_scrapping_stimuli/', stimuli)\n",
    "    scrapped_data_pictures = sorted(\n",
    "    [\n",
    "        os.path.join(scrapped_data_folder, sname)\n",
    "        for sname in os.listdir(scrapped_data_folder)\n",
    "    ]\n",
    "    )\n",
    "    for image in scrapped_data_pictures[:30]:\n",
    "        if os.path.basename(image) == \".DS_Store\":\n",
    "            continue\n",
    "        im = Image.open(image)\n",
    "        im = im.convert(\"RGB\")\n",
    "        im.save(os.path.join(data_path, f'new_celebA/{counter_images}.jpg'))\n",
    "        list_row = [ f'{counter_images}.jpg', new_id]\n",
    "        data.loc[len(data)] = list_row\n",
    "        counter_images+=1\n",
    "    stimuli_ids.append(new_id)\n",
    "    new_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a79558d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../../files/identity_CelebA_new.txt', header=None, index=None, sep=' ', mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33d6f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data[~data[1].isin(stimuli_ids)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0f9be59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import proj_path\n",
    "dir_path = os.path.join(proj_path, \"files/\")\n",
    "from data_processing_scripts.celebA_select_ids import clean_csv, create_csv, correct_gender, generate_new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6e6fd3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = pd.read_csv('../../files/identity_CelebA.txt', sep=\" \", header=None)\n",
    "old_data.columns =[\"name\", \"id\"]\n",
    "list_attr_celeba = pd.read_csv('../../files/list_attr_celeba.csv')\n",
    "list_attr_celeba = pd.merge(old_data, list_attr_celeba, on='name', how='outer')\n",
    "list_attr_celeba = list_attr_celeba[~list_attr_celeba[\"id\"].isin(stimuli_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6bbcffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurence of each id in the dataset\n",
    "id_occurence=list_attr_celeba['id'].value_counts()\n",
    "id_occurence=id_occurence.to_frame().reset_index()\n",
    "id_occurence.columns=['id', 'values']\n",
    "\n",
    "# Merge the id_occurence dataframe and merged dataframe\n",
    "merged = pd.merge(list_attr_celeba, id_occurence, on=\"id\")\n",
    "\n",
    "# Corret some mistakes in gender attribue\n",
    "merged=correct_gender(merged)\n",
    "\n",
    "df_male=merged[merged['Male']==1]\n",
    "df_female=merged[merged['Male']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8384acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final=create_csv(30, 435, df_male, df_female, merged)\n",
    "not_included = data[data[1].isin(stimuli_ids)].reset_index(drop=True)\n",
    "not_included.columns = [\"name\", \"id\"]\n",
    "final = pd.concat([final, not_included])\n",
    "final=generate_new_ids(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d73067a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"../../files/csv_files/final_celebA_with_stimuli.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
